# LAB 3-1: GestiÃ³n de archivos en HDFS y S3

| InformaciÃ³n |  |
| --- | --- |
| Materia | TÃ³picos especiales en TelemÃ¡tica |
| Curso | ST0263 |
| Estudiante | Santiago Puerta (mailto:spuertaf@eafit.edu.co) |
| Profesor | Edwin Nelson Montoya Munera (mailto:emontoya@eafit.edu.co) |

# 1. Objetivo

Copiar todos los archivos disponibles en [ğŸ˜¾ Datasets](https://github.com/st0263eafit/st0263-232/tree/main/bigdata/datasets) tanto HDFS (almacenamiento temporal) como en S3 (almacenamiento permanente).

---

# 2. Aspectos solucionados y no solucionados

- [x]  CreaciÃ³n de un bucket publico AWS S3.
- [x]  GestiÃ³n de archivos en HDFS vÃ­a terminal.
- [x]  GestiÃ³n de archivos en HDFS vÃ­a HUE.
- [x]  GestiÃ³n de arvhivos en S3 vÃ­a HUE.

---

# 5. Ambiente de ejecuciÃ³n

## GuÃ­a de uso

### Parte 1: CreaciÃ³n de un bucket publico de AWS S3

1. Entrar a la consola AWS y buscar el servicio S3:
2. Seleccionar el botÃ³n â€˜Crear bucketâ€™:
3. Primeramente registrar un nombre para el bucket;  posteriormente en la secciÃ³n â€˜****Object Ownership****â€™ seleccionar la opcion â€˜ACLs enabledâ€™ y seleccionar la casilla â€˜Object writerâ€™. 
    
    Seguidamente en la secciÃ³n â€˜**Block Public Access settings for this bucket**â€™ quitar el â€˜âœ… checkâ€™ a la opciÃ³n â€˜Block all public accessâ€™ y seleccionar la opciÃ³n â€˜I acknowledge that the current settings might result in this bucket and the objects within becoming public.â€™:
    
    ![Untitled](LAB%203-1%20Gestio%CC%81n%20de%20archivos%20en%20HDFS%20y%20S3%20928fbcdfe81343a2abe05fdd349afe93/Untitled.png)
    
    ![Untitled](LAB%203-1%20Gestio%CC%81n%20de%20archivos%20en%20HDFS%20y%20S3%20928fbcdfe81343a2abe05fdd349afe93/Untitled%201.png)
    
    ![Untitled](LAB%203-1%20Gestio%CC%81n%20de%20archivos%20en%20HDFS%20y%20S3%20928fbcdfe81343a2abe05fdd349afe93/Untitled%202.png)
    
4. Dejar las posteriores opciones por defecto y seleccionar el botÃ³n â€˜Crear Bucketâ€™:
    
    ![Untitled](LAB%203-1%20Gestio%CC%81n%20de%20archivos%20en%20HDFS%20y%20S3%20928fbcdfe81343a2abe05fdd349afe93/Untitled%203.png)
    
5. Una vez en el menÃº de S3, seleccionar el nombre del bucket anteriormente creado:
6. Seleccionar el apartado de â€˜Permissionsâ€™ y buscar la secciÃ³n â€˜**Access control list (ACL)**â€™, una vez allÃ­, seleccionar el botÃ³n â€˜Editâ€™.
7. Marcar con â€˜âœ…checkâ€™ las casillas â€˜Listâ€™ y â€˜Readâ€™ de â€˜Everyone (public access)â€™ y â€˜Authenticated users group (anyone with an AWS account)â€™; posteriormente marcar con â€˜âœ… checkâ€™ la opciÃ³n â€˜I understand the effects of these changes on my objects and buckets.â€™; finalmente seleccionar el botÃ³n â€˜Save changesâ€™:
    
    ![Untitled](LAB%203-1%20Gestio%CC%81n%20de%20archivos%20en%20HDFS%20y%20S3%20928fbcdfe81343a2abe05fdd349afe93/Untitled%204.png)
    
8. Ingresa al siguiente enlace [âœˆï¸ airlines](https://github.com/st0263eafit/st0263-232/blob/main/bigdata/datasets/airlines.csv) y selecciona el botÃ³n â€˜Download raw fileâ€™:
    
    ![Untitled](LAB%203-1%20Gestio%CC%81n%20de%20archivos%20en%20HDFS%20y%20S3%20928fbcdfe81343a2abe05fdd349afe93/Untitled%205.png)
    
9. Regresa a la interfaz principal de los buckets, selecciona el nombre del bucket anteriormente creado y arrastra el archivo descargado a el; seguidamente, selecciona el botÃ³n â€˜Uploadâ€™
    
    ![Untitled](LAB%203-1%20Gestio%CC%81n%20de%20archivos%20en%20HDFS%20y%20S3%20928fbcdfe81343a2abe05fdd349afe93/Untitled%206.png)
    
10. Selecciona el nombre del archivo anteriormente cargado y posteriormente, en el apartado â€˜Propertiesâ€™ en la secciÃ³n â€˜****Object overview****â€™ copia el â€˜Object URLâ€™:
    
    ![Untitled](LAB%203-1%20Gestio%CC%81n%20de%20archivos%20en%20HDFS%20y%20S3%20928fbcdfe81343a2abe05fdd349afe93/Untitled%207.png)
    
11. En una ventana de tu navegador pega la URL anteriormente copiada dejando de lado â€˜/airlines.csvâ€™:
    
    ![Untitled](LAB%203-1%20Gestio%CC%81n%20de%20archivos%20en%20HDFS%20y%20S3%20928fbcdfe81343a2abe05fdd349afe93/Untitled%208.png)
    

âœ… Â¡Listo! Ya posee un bucket publico de AWS S3

â€¼ï¸ Nota: Para poder leer los archivos del bucket publico creado anteriormente por medio de la CLI de AWS se puede usar el siguiente comando:

```bash
aws s3 ls s3://nombre-del-bucket
```

```bash
#Comando para leer el bucket anteriormente creado
aws s3 ls s3://datasetsspuertaf
```

---

### Parte 2: GestiÃ³n de archivos en HDFS vÃ­a terminal

Nota: 

1. Primeramente deberemos de descargar el archivo â€˜[ğŸ˜¾ datasets.zip](https://github.com/spuertaf/spuertaf-st0263/blob/main/Lab3-1/datasets.zip)â€™ y seleccionar el botÃ³n â€˜Download raw fileâ€™:
    
    ![Untitled](LAB%203-1%20Gestio%CC%81n%20de%20archivos%20en%20HDFS%20y%20S3%20928fbcdfe81343a2abe05fdd349afe93/Untitled%209.png)
    
2. Una vez descargado el archivo se deberÃ¡ de descomprimir el mismo.
3. Seguidamente deberemos de crear un clÃºster AWS EMR, visite la siguiente guÃ­a â€˜[CreaciÃ³n de un ClÃºster EMR](https://github.com/spuertaf/spuertaf-st0263/tree/main/Lab3-0)â€™ con este propÃ³sito.
4. Una vez creado el clÃºster especificado anteriormente debemos de conectarnos a el por medio de SSH; el como conectarse al clÃºster por medio de SSH puede ser encontrado en la guÃ­a anterior en el apartado â€˜[Parte 5: Entrando por SSH al nodo maestro](https://github.com/spuertaf/spuertaf-st0263/tree/main/Lab3-0)â€™.
5. Luego de establecer una conexiÃ³n con el nodo maestro crearemos una carpeta llamada â€˜gutenberg-smallâ€™ dentro de la ruta â€˜/user/hadoop/datasetsâ€™ por medio de los siguientes comandos:
    
    ```bash
    #crear el directorio 'datasets' dentro de la ruta 'user/hadoop/'
    hdfs dfs -mkdir /user/hadoop/datasets
    ```
    
    ```bash
    #crear el directorio 'gutenber-small' dentro de la ruta 'user/hadoop/datasets/'
    hdfs dfs -mkdir /user/hadoop/datasets/gutenberg-small
    ```
    
    ```bash
    #listar directorios y archivos presentes en la ruta /user/hadoop/
    hdfs dfs -ls /user/hadoop/datasets
    ```
    
    ![Untitled](LAB%203-1%20Gestio%CC%81n%20de%20archivos%20en%20HDFS%20y%20S3%20928fbcdfe81343a2abe05fdd349afe93/Untitled%2010.png)
    
6. Ahora copiaremos el contenido de la carpeta local â€˜datasets/gutenberg-smallâ€™ al directorio â€˜/user/hadoop/datasets/gutenberg-smallâ€™ por medio del siguiente comando:
    
    ```bash
    hdfs dfs -put /home/spuertaf/Desktop/datasets/gutenberg-small/*.txt /user/hadoop/datasets/gutenberg-small/
    ```
    

âœ… Â¡Listo! Ya puede subir archivos a HDFS del clÃºster EMR por medio de la terminal.

---

### Parte 3: GestiÃ³n de archivos en HDFS vÃ­a HUE

1. Entrar a la consola AWS y buscar el servicio EMR.
2. Seleccionar el â€˜ID del clÃºsterâ€™ que tenga el estado â€˜Esperando; luego, seleccione la opciÃ³n â€˜**Aplicaciones**â€™.
3. Seleccionar la URL del campo â€˜HUEâ€™ e ingresar el usuario â€˜hadoopâ€™ y una contraseÃ±a de su gusto.
4. Seleccionar el apartado â€˜Filesâ€™:
    
    ![Untitled](LAB%203-1%20Gestio%CC%81n%20de%20archivos%20en%20HDFS%20y%20S3%20928fbcdfe81343a2abe05fdd349afe93/Untitled%2011.png)
    
5. Seguidamente, seleccionar el botÃ³n â€˜Newâ€™ seleccionar la opciÃ³n â€˜Directoryâ€™:
    
    ![Untitled](LAB%203-1%20Gestio%CC%81n%20de%20archivos%20en%20HDFS%20y%20S3%20928fbcdfe81343a2abe05fdd349afe93/Untitled%2012.png)
    
6. Crear el directorio â€˜datasetsâ€™ si este aun no se encuentra creado:
    
    ![Untitled](LAB%203-1%20Gestio%CC%81n%20de%20archivos%20en%20HDFS%20y%20S3%20928fbcdfe81343a2abe05fdd349afe93/Untitled%2013.png)
    
7. En la ruta /user/admin/datasets/ crear el directorio onu/:
    
    ![Untitled](LAB%203-1%20Gestio%CC%81n%20de%20archivos%20en%20HDFS%20y%20S3%20928fbcdfe81343a2abe05fdd349afe93/Untitled%2014.png)
    
8. Seleccionar el botÃ³n â€˜Uploadâ€™ una vez en la ruta â€˜/user/admin/datasets/onuâ€™:
    
    ![Untitled](LAB%203-1%20Gestio%CC%81n%20de%20archivos%20en%20HDFS%20y%20S3%20928fbcdfe81343a2abe05fdd349afe93/Untitled%2015.png)
    
    ![Untitled](LAB%203-1%20Gestio%CC%81n%20de%20archivos%20en%20HDFS%20y%20S3%20928fbcdfe81343a2abe05fdd349afe93/Untitled%2016.png)
    
9. Selecciona los archivos presenes en local de la carpeta onu/; posteriormente, seleccionar el botÃ³n â€˜openâ€™:
    
    ![Untitled](LAB%203-1%20Gestio%CC%81n%20de%20archivos%20en%20HDFS%20y%20S3%20928fbcdfe81343a2abe05fdd349afe93/Untitled%2017.png)
    

âœ… Â¡Listo! Ya puede subir archivos a HDFS del clÃºster EMR por medio de HUE.

---

### Parte 4: GestiÃ³n de archivos en S3 vÃ­a HUE

1. Entrar a la consola AWS y buscar el servicio EMR.
2. Seleccionar el â€˜ID del clÃºsterâ€™ que tenga el estado â€˜Esperando; luego, seleccione la opciÃ³n â€˜**Aplicaciones**â€™.
3. Seleccionar la URL del campo â€˜HUEâ€™ e ingresar el usuario â€˜hadoopâ€™ y una contraseÃ±a de su gusto.
4. Seleccionar el apartado â€˜S3â€™:
    
    ![Untitled](LAB%203-1%20Gestio%CC%81n%20de%20archivos%20en%20HDFS%20y%20S3%20928fbcdfe81343a2abe05fdd349afe93/Untitled%2018.png)
    
5. Seleccionar el nombre del Bucket creado en la â€˜Parte 1: CreaciÃ³n de un bucket publico de AWS S3â€™, en este caso particular el Bucket â€˜datasetsspuertafâ€™.
6. Seleccionar el botÃ³n â€˜Uploadâ€™, â€˜Select Filesâ€™; y finalmente elegir los archivos que se deseen cargar.

âœ… Â¡Listo! Ya puede subir archivos a un bucket de S3 vÃ­a HUE